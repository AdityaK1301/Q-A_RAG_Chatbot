{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f5b12ba-23ef-49e7-b069-d697f9cdefa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: pymupdf in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.33.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests sentence-transformers pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e70a2c67-d77d-4b49-9b35-7b4e434986b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import fitz  \n",
    "import requests\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c230ed5-5b01-4dba-ad3d-642b2086863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "books_folder = \"class3_books\"  \n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
    "OLLAMA_MODEL = \"gemma:2b-instruct\"\n",
    "\n",
    "print(\"Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01d8c582-9884-4b2e-baac-c9b5e3c6a5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf_bytes(pdf_bytes):\n",
    "    doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "    text = []\n",
    "    for page in doc:\n",
    "        text.append(page.get_text())\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "def clean_text_generic(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s\\.\\,\\?\\!\\:\\;\\-]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text(text, max_words=200):\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    chunks = []\n",
    "    for para in paragraphs:\n",
    "        words = para.split()\n",
    "        if len(words) == 0:\n",
    "            continue\n",
    "        for i in range(0, len(words), max_words):\n",
    "            chunks.append(\" \".join(words[i:i+max_words]))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d1c08e8-854f-4302-880e-0f257c928f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AI_base.zip\n",
      "Processed ENGLISH.zip\n",
      "Processed EVS.zip\n",
      "Processed MATHS.zip\n",
      "Total chunks collected: 505\n",
      "Embeddings generated successfully!\n"
     ]
    }
   ],
   "source": [
    "all_chunks = []\n",
    "for zip_file in os.listdir(books_folder):\n",
    "    if zip_file.endswith(\".zip\"):\n",
    "        zip_path = os.path.join(books_folder, zip_file)\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "            for pdf_name in zf.namelist():\n",
    "                if pdf_name.endswith(\".pdf\"):\n",
    "                    with zf.open(pdf_name) as pdf_file:\n",
    "                        pdf_bytes = pdf_file.read()\n",
    "                        text = extract_text_from_pdf_bytes(pdf_bytes)\n",
    "                        text = clean_text_generic(text)\n",
    "                        chunks = chunk_text(text)\n",
    "                        tagged = [f\"[{pdf_name}] {ch}\" for ch in chunks]\n",
    "                        all_chunks.extend(tagged)\n",
    "        print(f\"Processed {zip_file}\")\n",
    "\n",
    "print(f\"Total chunks collected: {len(all_chunks)}\")\n",
    "\n",
    "# Generate embeddings\n",
    "if all_chunks:\n",
    "    chunk_embeddings = embedding_model.encode(all_chunks, convert_to_tensor=True)\n",
    "    print(\"Embeddings generated successfully!\")\n",
    "else:\n",
    "    print(\"No syllabus content found. The system will still work but won't have context.\")\n",
    "    chunk_embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "637772d8-9e33-47ee-9019-b63884b0ad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query, k=3):\n",
    "    if not all_chunks:\n",
    "        return \"No syllabus content available.\"\n",
    "    \n",
    "    query_embedding = embedding_model.encode([query], convert_to_tensor=True)\n",
    "    hits = util.semantic_search(query_embedding, chunk_embeddings, top_k=k)[0]\n",
    "    top_chunks = [all_chunks[h[\"corpus_id\"]] for h in hits]\n",
    "    return \"\\n\".join(top_chunks)\n",
    "\n",
    "def query_ollama(prompt, max_tokens=150):\n",
    "    payload = {\n",
    "        \"model\": OLLAMA_MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.1,\n",
    "            \"num_predict\": max_tokens,\n",
    "            \"top_p\": 0.9,\n",
    "            \"repeat_penalty\": 1.1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(OLLAMA_API_URL, json=payload, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"response\"]\n",
    "    except requests.exceptions.RequestException:\n",
    "        return \"I'm having trouble connecting to the knowledge base right now.\"\n",
    "\n",
    "def answer_question(query):\n",
    "    context = retrieve_context(query, k=3)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a teacher for Class 3 students.\n",
    "    Answer the question using the following syllabus context only.\n",
    "    If the context does not have the answer, say \"I couldn't find this in the syllabus.\"\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Context from syllabus:\n",
    "    {context}\n",
    "\n",
    "    Answer in simple words for a Class 3 student:\n",
    "    \"\"\"\n",
    "    \n",
    "    return query_ollama(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "224d7e0f-e5b7-47b1-a373-1c3b9f59686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chat():\n",
    "    print(\"\\n🤖 CBSE Class 3 Q/A Tool Ready! Ask syllabus questions (type 'quit' to exit)\\n\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() in [\"quit\", \"exit\"]:\n",
    "            break\n",
    "        print(\"Bot:\", answer_question(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d79688a1-c3dd-4caf-ae16-9e3555b459a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 CBSE Class 3 Q/A Tool Ready! Ask syllabus questions (type 'quit' to exit)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what are word numerals\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: **Word numerals are words that represent numbers.**\n",
      "\n",
      "For example, the number 56 has the word \"fifty-six\" written next to it.\n",
      "\n",
      "We can also make new words with numbers, like 15, 27, and 94.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  why was meena angry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: **1. What was Meena playing with?**\n",
      "Meena made paper boats.\n",
      "\n",
      "\n",
      "**2. Meena was angry. Why?**\n",
      "Meena was angry because someone was pushing her paper boats into the water.\n",
      "\n",
      "\n",
      "**3. How did Meena help the boy? B.**\n",
      "Meena helped the boy by teaching him how to make paper boats.\n",
      "\n",
      "\n",
      "**4. What else can you make with paper? Discuss in small groups.**\n",
      "You can make other things with paper, such as cards, hats, and boxes.\n",
      "\n",
      "\n",
      "**5. What games do you play with your friends? Which one do you like the most? Why?**\n",
      "We can play games with our friends like tag, hide and seek, and\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    }
   ],
   "source": [
    "run_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f97f1f-bce2-491c-91fb-acacd7e717ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
